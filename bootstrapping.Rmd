---
title: "The Bootstrap"
author: "John Clements"
date: "2/26/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Requirements

- `dplyr`: for general data manipulation
- `parallel`: for parallel computation
- `tidyquant`: for access to financial data
- `lubridate`: for date manipulation
- `hyfo`: for resampling frequency of time-series
- `ggplot2`: for data visualization

```{r include=FALSE}
suppressMessages(library(dplyr))
suppressMessages(library(parallel))
suppressMessages(library(tidyquant))
suppressMessages(library(lubridate))
suppressMessages(library(hyfo))
suppressMessages(library(ggplot2))
```


# I. Introduction

When estimating a statistic of interest, we are not only interested in the point-value of that statistic. We also want to quantify our uncertainty about our estimate

# II. Technical Implementation

While there are libraries like [`bootstrap`](https://cran.r-project.org/web/packages/bootstrap/bootstrap.pdf) that do bootstrapping for us, I find coding lightweight functions a great learning method, even though I use well maintained libraries in practice.

To demonstrate bootstrapping, I wrote two main functions functions and two helpers.

## A. Single Bootstrap Function

The function `one_bootstrap` is a helper function that takes in a data vector and a function to estimate a statistic of interest and computes the quantity of interest on a single bootstrapped sample.

```{r}
one_bootstrap <- function(my_vec, my_func){
  ###
  # This function retrieves one bootstrapped sample and returns the statistic
  # of interest.
  #
  # Args
  # ----
  # my_vec : numeric vector
  #   A vector of numbers of which to compute the statistic of interest.
  # my_func : function
  #   Function which computes the statistic of interest.
  #
  # Returns
  # -------
  # double
  #   The statistic of interest computed on the bootstrapped sample.
  #
  ###
  bootstrapped_sample <- sample(my_vec, size=length(my_vec), replace=TRUE)
  return(my_func(bootstrapped_sample))
}
```

## B. Bootstrapping using Looping

The function `bootstrap_replicate` takes in a data vector, function, and the number of bootstrap iterations to perform, **B** and returns a list with the mean, standard error, and estimates themselves. It does this by calling `one_bootstrap` **B** times. R's `replicate` function performs loops in a faster manner than explicitly writing a for loop.

```{r}
bootstrap_replicate <- function(my_vec, my_func, B){
  ###
  # This function takes in a data vector, function, and the number of bootstrap
  # iterations and returns a list holding the mean and standard deviation of the 
  # bootstrap estimates as well, as the vector of the bootstrap estimates. It
  # utilizes the replicate function for optimized looping.
  #
  # Args
  # ----
  # my_vec : numeric vector
  #   A vector of numbers of which to compute the statistic of interest.
  # my_func : function
  #   Function which computes the statistic of interest.
  # B : int
  #   The number of bootstrapped samples to return.
  #
  # Returns
  # -------
  # output : list
  #   A list of the mean, and standard deviation of the estimates and a vector
  #   of the estimates.
  #
  ###
  estimates <- replicate(B, one_bootstrap(my_vec, my_func))
  output <- list(
    'mean' = mean(estimates),
    'se' = sd(estimates),
    'estimates' = estimates
  )
  return(output)
}
```

## C. Parallelized Bootstrapping

Processing each bootstrapped sample does not depend on the other samples, allowing bootstrapping to be easily parallelized, which is what `bootstrap_parallel` does.

```{r}
bootstrap_replicate_par <- function(B, my_vec, my_func){
  ###
  # This function is a helper function for the parallized bootstrapping function.
  # It takes in a vector whose length determines the number of bootstrap samples
  # to take, a data vector, and a function. It returns the list of bootstrapped
  # estimates from my_func.
  #
  # Args
  # ----
  # B : vector
  #   A vector whose length determines of bootstrapped samples to return.
  # my_vec : numeric vector
  #   A vector of numbers of which to compute the statistic of interest.
  # my_func : function
  #   Function which computes the statistic of interest.
  #
  # Returns
  # -------
  # estimates : vector
  #   A vector of the estimates.
  #
  ###
  estimates <- replicate(length(B), one_bootstrap(my_vec, my_func))
  return(estimates)
}
```

```{r}
bootstrap_parallel <- function(my_vec, my_func, B){
  ###
  # This function takes in a data vector, function, and the number of bootstrap
  # iterations and returns a list holding the mean and standard deviation of the 
  # bootstrap estimates as well, as the vector of the bootstrap estimates. It
  # utilizes parallel computing.
  #
  # Args
  # ----
  # my_vec : numeric vector
  #   A vector of numbers of which to compute the statistic of interest.
  # my_func : function
  #   Function which computes the statistic of interest.
  # B : int
  #   The number of bootstrapped samples to return.
  #
  # Returns
  # -------
  # output : list
  #   A list of the mean, and standard deviation of the estimates and a vector
  #   of the estimates.
  #
  ###
  
  # Count the cores and make a cluster from leaving one core free.
  cores <- detectCores()  
  cluster <- makeCluster(cores - 1)

  # Create a vector that will be split up and determine the number of bootstrap
  # samples to get on each core.
  boot_vec <- 1:B
  
  # Export functions to the cluster.
  clusterExport(
    cluster, 
    list("boot_vec", "one_bootstrap", "bootstrap_replicate_par", "my_vec",
         "my_func"),
    envir=environment()
    )

  estimates <- parSapply(
    cluster,
    boot_vec,
    FUN=bootstrap_replicate_par,
    my_vec=my_vec,
    my_func=my_func
    )

  stopCluster(cluster)

  output <- list(
    'mean' = mean(estimates),
    'se' = sd(estimates),
    'estimates' = estimates
  )

  return(output)
}
```


# Applications

The data set I am using is the daily returns of the S&P 500 from the beginning of 2000 to the end of last month (January 2022). I lose a day at the very beginning due to calculating returns. I also downloaded the 3-month Treasure interest rates, converting BPS to decimals and converting annual interest rates to daily rates to use for the Sharpe Ratio demonstration.

## A. Data Set-up

Below is a plot of daily returns. We can see daily returns are peaked slightly to the right of 0. The tails are long, with the left-tail a bit longer than the right, meaning there have been bigger losses on the worst days than the biggest gains on the best days, which is common in financial data.

```{r echo=FALSE}
start_date <- '2000-01-01'
end_date <- '2022-01-31'

# Read in the 3-month treasury interest rates, convert the DATE column to date
# data, subset for the time-period of interest, resample to daily, and convert
# annual rates in BPS to daily decimal values.
rf_df <- read.csv('data/TB3MS.csv')
rf_df$DATE <- as_date(rf_df$DATE)
rf_df <- rf_df[rf_df$DATE >= as_date(start_date) 
               & rf_df$DATE <= as_date(end_date),]
rf_df <- resample(rf_df, 'mon2day')
rf_df$value <- as.numeric(rf_df$value) / 100 / 365
rownames(rf_df) <- rf_df$Date

# Load S&P 500 data for the dates of interest and calculate the daily returns.
getSymbols(
  "^GSPC",
  from=start_date,
  to=end_date,
  warnings=FALSE,
  auto.assign=TRUE
  )
adj_close <- as.vector(GSPC$GSPC.Adjusted)
trading_days <- length(adj_close)
returns <- tail(adj_close, trading_days-1) / head(adj_close, trading_days-1) - 1
GSPC$returns <- c(NA, returns)
GSPC <- as.data.frame(tail(GSPC, trading_days-1))

# Filter rf_df for the dates in GSPC, add the risk-free rates to the dataframe
# and calculate the excess returns.
rf_df <- rf_df %>% filter(rownames(rf_df) %in% rownames(GSPC))
GSPC$rf <- rf_df$value
GSPC$ex_ret <- GSPC$returns - GSPC$rf

# Reset the start and end-dates to the first and last dates in the GSPC 
# dataframe.
new_start <- rownames(head(GSPC, 1))
new_end <- rownames(tail(GSPC, 1))
```

```{r include=FALSE}
# Create a histogram of daily returns with a KDE overlayed.
ggplot(GSPC, aes(x=returns)) +
  geom_histogram(aes(y = after_stat(density)),
                 color='darkblue',
                 fill='lightblue',
                 position = 'identity',
                 bins=30) +
  geom_density(bw=0.0025, color='red', size=1) + 
  labs(
    title=paste0('S&P 500 Daily Returns (', 
                 new_start, ' to ', new_end, ')'),
    x='returns',
    y='density'
    ) +
    geom_vline(xintercept=mean(returns),
             linetype='dashed',
             color='black',
             size=1)
```

Now let's look at the Sharpe Ratio!

## B. Sharpe Ratio

The Sharpe Ratio is the expected returns in excess of the risk-free rate-of-return per unit of risk. Risk is measured by the standard deviation of returns.

$$SR = \frac{E[R - r_f]}{\sigma_R}$$

The standard error of the Sharpe Ratio is computed as:

$$\hat{SE}(SR) = \sqrt{\frac{1 + \frac{1}{2}\hat{SR}^2}{n}}$$

We can estimate the Sharpe Ratio with the sample mean of excess returns and standard deviations of the returns.

THe function below is calculates the Sharpe ratio given the returns and risk-free rate.

```{r}
sharpe_ratio <- function(returns, rf=GSPC$rf){
  return(mean(returns-rf)/sd(returns))
}
```

The way I wrote it is not conducive to use with my parallelized function because it cannot export the the GSPC dataframe to the cluster, so we will use the bootstrapping function utilizing optimized looping.

```{r}
set.seed(1)
B <- 5000

sharpe_est_boot <- bootstrap_replicate(GSPC$returns, sharpe_ratio, B)
```

Below is a table comparing the estimates for the Sharpe Ratio, and the estimated standard error and 95% Confidence Intervals (CIs). The CIs are only approximate because the ones in the first row rely on the Central Limit Theorem and the second are from the bootstrap estimate quantiles.

```{r include=FALSE}
sharpe_est <- sharpe_ratio(GSPC$returns)
sharpe_se <- sqrt((1+0.5*(sharpe_est^2))/nrow(GSPC))
lower_apprx <- as.numeric(sharpe_est + qnorm(0.025)*sharpe_se)
upper_apprx <- as.numeric(sharpe_est + qnorm(0.975)*sharpe_se)

lower_boot <- as.numeric(quantile(sharpe_est_boot$estimates, 0.025))
upper_boot <- as.numeric(quantile(sharpe_est_boot$estimates, 0.975))

labels <- c('MLE', 'Bootstrapped')
mean_ests <- c(sharpe_est, sharpe_est_boot$mean)
se_ests <- c(sharpe_se, sharpe_est_boot$se)
lower <- c(lower_apprx, lower_boot)
upper <- c(upper_apprx, upper_boot)
results_df <- as.data.frame(
  list('Method'=labels, 'Est'=mean_ests, 'SE'=se_ests, 'Lower'=lower, 
       'Upper'=upper)
  )

knitr::kable(
  results_df,
  caption='Comparison of Estimates of Sharpe Ratio w/ 95 % CIs'
  )
```

The values are fairly close, but the lower bound is a little more negative in the bootstrapped estimates.

The histogram below contains the bootstrapped estimates of the Sharpe Ratio with the approximate CI (solid black lines) and the bootstrapped CI (dashed red line) over-layed. We see the that the bootstrapped CI is a little wider and shifted to the left of the approximate CI.

```{r incude=FALSE}
bootstrap_sharpe_df <- as.data.frame(list('sharpe'=sharpe_est_par$estimates))

ggplot(bootstrap_sharpe_df, aes(x=sharpe)) +
  geom_histogram(
    aes(y=(..density..)),
    color='darkblue',
    fill='lightblue',
    position = 'identity',
    bins=30) +
  labs(
    title='Bootstrapped Distribution of Sharpe Ratio w/ 95% CIs',
    x='Sharpe ratio',
    y='density'
    ) +
  geom_vline(xintercept=sharpe_lower,
             linetype='solid',
             color='black',
             size=1) +
  geom_vline(xintercept=sharpe_higher,
             linetype='solid',
             color='black',
             size=1) + 
  geom_vline(xintercept=lower_boot,
             linetype='dashed',
             color='red',
             size=1) +
  geom_vline(xintercept=upper_boot,
             linetype='dashed',
             color='red',
             size=1)

```




## B. Value at Risk and Expected Shortfall

```{r}
non_parametric_var <- function(returns, prob=0.05, holdings=1000000){
  return(as.numeric(-holdings*quantile(returns, probs=prob)))
}

non_parametric_es <- function(returns, prob=0.05, holdings=1000000){
  below_quantile <- returns < quantile(returns, probs=prob)
  numer <- sum(returns[below_quantile])
  denom <- sum(below_quantile)
  return(holdings * (numer / denom))
}

var_est <- bootstrap_parallel(returns, non_parametric_var, 100000)
es_est <- bootstrap_parallel(returns, non_parametric_es, 100000)
```


```{r include=FALSE}
var_est_df <- as.data.frame(list('VaR'=var_est$estimates))

var_hist <- ggplot(var_est_df, aes(x=VaR)) +
  geom_histogram(aes(y = after_stat(density)),
                 color='darkblue',
                 fill='lightblue',
                 position = 'identity',
                 bins=30) +
  labs(
    title='5% VaR for $1,000,000 w/ 95% Bootstrapped CIs',
    x='VaR',
    y='density'
    ) +
  geom_vline(xintercept=var_est$mean, linetype='dashed', size=1) +
  geom_vline(xintercept=quantile(var_est$estimates, 0.025), linetype='dashed',
             color='red',
             size=1) +
  geom_vline(xintercept=quantile(var_est$estimates, 0.975), linetype='dashed',
             color='red',
             size=1)

var_hist
```

```{r include=FALSE}
es_est_df <- as.data.frame(list('ES'=es_est$estimates))

es_hist <- ggplot(es_est_df, aes(x=ES)) +
  geom_histogram(aes(y = after_stat(density)),
                 color='darkblue',
                 fill='lightblue',
                 position = 'identity',
                 bins=30) +
  labs(
    title='5% Expected Shortfall for $1,000,000 w/ 95% Bootstrapped CIs',
    x='expected shortfall',
    y='density'
    ) +
  geom_vline(xintercept=es_est$mean, linetype='dashed', size=1) +
  geom_vline(xintercept=quantile(es_est$estimates, 0.025), linetype='dashed',
             color='red',
             size=1) +
  geom_vline(xintercept=quantile(es_est$estimates, 0.975), linetype='dashed',
             color='red',
             size=1)

es_hist
```


