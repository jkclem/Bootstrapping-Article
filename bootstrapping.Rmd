---
title: "The Bootstrap"
author: "John Clements"
date: "2/26/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Requirements

- `dplyr`: for general data manipulation
- `parallel`: for parallel computation
- `tidyquant`: for access to financial data
- `lubridate`: for date manipulation
- `hyfo`: for resampling frequency of time-series
- `ggplot2`: for data visualization

```{r include=FALSE}
suppressMessages(library(dplyr))
suppressMessages(library(parallel))
suppressMessages(library(tidyquant))
suppressMessages(library(lubridate))
suppressMessages(library(hyfo))
suppressMessages(library(ggplot2))
```


# I. Introduction

When estimating a statistic of interest, we are not only interested in the point-value of that statistic. We also want to quantify our uncertainty about our estimate. For standard quantities of interest, like the mean or standard deviation,
we have analytical results for the [sampling distributions](https://en.wikipedia.org/wiki/Sampling_distribution) or can easily use large-sample approximations. Often times, we are interested in quantities where we do not know the sampling distribution and deriving it is difficult, if it is even possible. In these cases, the bootstrap is a valuable tool for quantifying uncertainty.

When bootstrapping, we treat our sample as the population. We repeatedly resample the same number of observations as the original sample *with replacement* and calculate the statistic of interst on those samples. We resample with replacement because the samples would not be independent when sampling without replacement. They would also exactly match our original sample, which would not be very helpful. This provides us with a simulated sampling distribution with which we can analyze.

# II. Implementation

While there are libraries like [`bootstrap`](https://cran.r-project.org/web/packages/bootstrap/bootstrap.pdf) that do bootstrapping for us, I find coding lightweight functions a great learning method, even though I use well maintained libraries in practice.

To demonstrate bootstrapping, I wrote two main functions functions and two helpers.

## A. Single Bootstrap Function

The function `one_bootstrap` is a helper function that takes in a data vector and a function to estimate a statistic of interest and computes the quantity of interest on a single bootstrapped sample.

```{r}
one_bootstrap <- function(my_vec, my_func){
  ###
  # This function retrieves one bootstrapped sample and returns the statistic
  # of interest.
  #
  # Args
  # ----
  # my_vec : numeric vector
  #   A vector of numbers of which to compute the statistic of interest.
  # my_func : function
  #   Function which computes the statistic of interest.
  #
  # Returns
  # -------
  # double
  #   The statistic of interest computed on the bootstrapped sample.
  #
  ###
  bootstrapped_sample <- sample(my_vec, size=length(my_vec), replace=TRUE)
  return(my_func(bootstrapped_sample))
}
```

## B. Bootstrapping using Looping

The function `bootstrap_replicate` takes in a data vector, function, and the number of bootstrap iterations to perform, **B** and returns a list with the mean, standard error, and estimates themselves. It does this by calling `one_bootstrap` **B** times. R's `replicate` function performs loops in a faster manner than explicitly writing a for loop.

```{r}
bootstrap_replicate <- function(my_vec, my_func, B){
  ###
  # This function takes in a data vector, function, and the number of bootstrap
  # iterations and returns a list holding the mean and standard deviation of the 
  # bootstrap estimates as well, as the vector of the bootstrap estimates. It
  # utilizes the replicate function for optimized looping.
  #
  # Args
  # ----
  # my_vec : numeric vector
  #   A vector of numbers of which to compute the statistic of interest.
  # my_func : function
  #   Function which computes the statistic of interest.
  # B : int
  #   The number of bootstrapped samples to return.
  #
  # Returns
  # -------
  # output : list
  #   A list of the mean, and standard deviation of the estimates and a vector
  #   of the estimates.
  #
  ###
  estimates <- replicate(B, one_bootstrap(my_vec, my_func))
  output <- list(
    'mean' = mean(estimates),
    'se' = sd(estimates),
    'estimates' = estimates
  )
  return(output)
}
```

## C. Parallelized Bootstrapping

Processing each bootstrapped sample does not depend on the other samples, as the samples are independent of one another. This allows bootstrapping to be easily parallelized, which is what `bootstrap_parallel` does.

```{r}
bootstrap_replicate_par <- function(B, my_vec, my_func){
  ###
  # This function is a helper function for the parallized bootstrapping function.
  # It takes in a vector whose length determines the number of bootstrap samples
  # to take, a data vector, and a function. It returns the list of bootstrapped
  # estimates from my_func.
  #
  # Args
  # ----
  # B : vector
  #   A vector whose length determines of bootstrapped samples to return.
  # my_vec : numeric vector
  #   A vector of numbers of which to compute the statistic of interest.
  # my_func : function
  #   Function which computes the statistic of interest.
  #
  # Returns
  # -------
  # estimates : vector
  #   A vector of the estimates.
  #
  ###
  estimates <- replicate(length(B), one_bootstrap(my_vec, my_func))
  return(estimates)
}
```

```{r}
bootstrap_parallel <- function(my_vec, my_func, B){
  ###
  # This function takes in a data vector, function, and the number of bootstrap
  # iterations and returns a list holding the mean and standard deviation of the 
  # bootstrap estimates as well, as the vector of the bootstrap estimates. It
  # utilizes parallel computing.
  #
  # Args
  # ----
  # my_vec : numeric vector
  #   A vector of numbers of which to compute the statistic of interest.
  # my_func : function
  #   Function which computes the statistic of interest.
  # B : int
  #   The number of bootstrapped samples to return.
  #
  # Returns
  # -------
  # output : list
  #   A list of the mean, and standard deviation of the estimates and a vector
  #   of the estimates.
  #
  ###
  
  # Count the cores and make a cluster from leaving one core free.
  cores <- detectCores()  
  cluster <- makeCluster(cores - 1)

  # Create a vector that will be split up and determine the number of bootstrap
  # samples to get on each core.
  boot_vec <- 1:B
  
  # Export functions to the cluster.
  clusterExport(
    cluster, 
    list("boot_vec", "one_bootstrap", "bootstrap_replicate_par", "my_vec",
         "my_func"),
    envir=environment()
    )

  estimates <- parSapply(
    cluster,
    boot_vec,
    FUN=bootstrap_replicate_par,
    my_vec=my_vec,
    my_func=my_func
    )

  stopCluster(cluster)

  output <- list(
    'mean' = mean(estimates),
    'se' = sd(estimates),
    'estimates' = estimates
  )

  return(output)
}
```

With our function defined, we can move onto applications

# III. Applications

To demonstrate bootstrapping, I will apply it to the [Sharpe ratio](), Value at Risk [VaR](https://en.wikipedia.org/wiki/Sharpe_ratio), and Expected Shortfall [ES](https://en.wikipedia.org/wiki/Expected_shortfall), sometimes known as Conditional Value at Risk or CVaR.

The data set I am using for this demonstration is the daily returns of the S&P 500 from the beginning of 2000 to the end of January 2022. I lose a day at the very beginning due to calculating returns. I also downloaded the 3-month Treasure interest rates, converting BPS to decimals and converted annual interest rates to daily rates to use for the Sharpe Ratio demonstration.

I am assuming returns are [*iid*](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) for simplicity, although this is a dubious assumption due to the possibility of correlations in returns and [volatility clustering](https://en.wikipedia.org/wiki/Volatility_clustering).

Below is a plot of daily returns. We can see daily returns are peaked slightly to the right of 0. The tails are long, with the left-tail a bit longer than the right, meaning there have been bigger losses on the worst days than the biggest gains on the best days, which is common in financial data.

```{r echo=FALSE}
start_date <- '2000-01-01'
end_date <- '2022-01-31'

# Read in the 3-month treasury interest rates, convert the DATE column to date
# data, subset for the time-period of interest, resample to daily, and convert
# annual rates in BPS to daily decimal values.
rf_df <- read.csv('data/TB3MS.csv')
rf_df$DATE <- as_date(rf_df$DATE)
rf_df <- rf_df[rf_df$DATE >= as_date(start_date) 
               & rf_df$DATE <= as_date(end_date),]
rf_df <- resample(rf_df, 'mon2day')
rf_df$value <- as.numeric(rf_df$value) / 100 / 365
rownames(rf_df) <- rf_df$Date

# Load S&P 500 data for the dates of interest and calculate the daily returns.
getSymbols(
  "^GSPC",
  from=start_date,
  to=end_date,
  warnings=FALSE,
  auto.assign=TRUE,
  "getSymbols.warning4.0"=FALSE
  )
adj_close <- as.vector(GSPC$GSPC.Adjusted)
trading_days <- length(adj_close)
returns <- tail(adj_close, trading_days-1) / head(adj_close, trading_days-1) - 1
GSPC$returns <- c(NA, returns)
GSPC <- as.data.frame(tail(GSPC, trading_days-1))

# Filter rf_df for the dates in GSPC, add the risk-free rates to the dataframe
# and calculate the excess returns.
rf_df <- rf_df %>% filter(rownames(rf_df) %in% rownames(GSPC))
GSPC$rf <- rf_df$value
GSPC$ex_ret <- GSPC$returns - GSPC$rf

# Reset the start and end-dates to the first and last dates in the GSPC 
# dataframe.
new_start <- rownames(head(GSPC, 1))
new_end <- rownames(tail(GSPC, 1))
```

```{r echo=FALSE}
# Create a histogram of daily returns with a KDE overlayed.
hist <- ggplot(GSPC, aes(x=returns)) +
  geom_histogram(aes(y = after_stat(density)),
                 color='darkblue',
                 fill='lightblue',
                 position = 'identity',
                 bins=30) +
  geom_density(bw=0.0025, color='red', size=1) + 
  labs(
    title=paste0('S&P 500 Daily Returns (', 
                 new_start, ' to ', new_end, ')'),
    x='returns',
    y='density'
    ) +
    geom_vline(xintercept=mean(returns),
             linetype='dashed',
             color='black',
             size=1)

hist
```

Now let's look at the Sharpe ratio!

## B. Sharpe Ratio

The Sharpe ratio is the expected returns in excess of the risk-free rate-of-return per unit of risk. Risk is measured by the standard deviation of returns. We can estimate the Sharpe Ratio with the sample mean of excess returns divided by the standard deviations of the returns.

$$SR = \frac{E[R - r_f]}{\sigma_R}$$

The standard error of the Sharpe Ratio is computed as:

$$\hat{SE}(SR) = \sqrt{\frac{1 + \frac{1}{2}\hat{SR}^2}{n}}$$

The function below is calculates the Sharpe ratio given the returns and risk-free rate.

```{r}
sharpe_ratio <- function(returns, rf=GSPC$rf){
  return(mean(returns-rf)/sd(returns))
}
```

The way I wrote it is not conducive to use with my parallelized function because it cannot export the the GSPC dataframe to the cluster, so we will use the bootstrapping function utilizing optimized looping.

```{r}
set.seed(1)
B <- 5000

sharpe_est_boot <- bootstrap_replicate(GSPC$returns, sharpe_ratio, B)
```

Below is a table comparing the estimates for the Sharpe Ratio, and the estimated standard error and 95% Confidence Intervals (CIs). The CIs are only approximate because the CI in the first row rely on the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) and the CI in the second row are from the bootstrap estimates quantiles.

```{r include=FALSE}
sharpe_est <- sharpe_ratio(GSPC$returns)
sharpe_se <- sqrt((1+0.5*(sharpe_est^2))/nrow(GSPC))
lower_apprx <- as.numeric(sharpe_est + qnorm(0.025)*sharpe_se)
upper_apprx <- as.numeric(sharpe_est + qnorm(0.975)*sharpe_se)

lower_boot <- as.numeric(quantile(sharpe_est_boot$estimates, 0.025))
upper_boot <- as.numeric(quantile(sharpe_est_boot$estimates, 0.975))

labels <- c('MLE', 'Bootstrapped')
mean_ests <- c(sharpe_est, sharpe_est_boot$mean)
se_ests <- c(sharpe_se, sharpe_est_boot$se)
lower <- c(lower_apprx, lower_boot)
upper <- c(upper_apprx, upper_boot)
results_df <- as.data.frame(
  list('Method'=labels, 'Est'=mean_ests, 'SE'=se_ests, 'Lower'=lower, 
       'Upper'=upper)
  )

knitr::kable(
  results_df,
  caption='Comparison of Estimates of Sharpe Ratio w/ 95 % CIs'
  )
```

The values are very close, but the estimate of the Sharpe ratio and the CI is slightly shifted right for the bootstrapped estimate. This difference is negligible.

The histogram below contains the bootstrapped estimates of the Sharpe Ratio with the approximate CI (solid black lines) and the bootstrapped CI (dashed red line) over-layed. We see the that the bootstrapped CI is not that different than the approximate CI.

```{r include=FALSE}
bootstrap_sharpe_df <- as.data.frame(list('sharpe'=sharpe_est_boot$estimates))

ggplot(bootstrap_sharpe_df, aes(x=sharpe)) +
  geom_histogram(
    aes(y=(..density..)),
    color='darkblue',
    fill='lightblue',
    position = 'identity',
    bins=30) +
  labs(
    title='Bootstrapped Distribution of Sharpe Ratio w/ 95% CIs',
    x='Sharpe ratio',
    y='density'
    ) +
  geom_vline(xintercept=lower_apprx,
             linetype='solid',
             color='black',
             size=1) +
  geom_vline(xintercept=upper_apprx,
             linetype='solid',
             color='black',
             size=1) + 
  geom_vline(xintercept=lower_boot,
             linetype='dashed',
             color='red',
             size=1) +
  geom_vline(xintercept=upper_boot,
             linetype='dashed',
             color='red',
             size=1)

```

Now let's look at VaR and ES!

## B. Value at Risk and Expected Shortfall

VaR and ES are in depth topics that I can not explore in this article, so I am only giving a basic overview.

VaR($\alpha$) is a measure of risk of an asset or portfolio. If VaR(0.05) is \$10,000, there is a 5% probability that we see a loss of *at least* \$10,000 in a given time period. You can assume a distribution of returns and estimate VaR based on that probabilistic model, or you can use the sample quantile if you have sufficient data. You really want to have enough data points to fully cover the left-tail of the returns distribution if taking this approach, which is the one I am taking. Choosing too small of an $\alpha$ relative to your sample size can lead to large underestimates of risk.

This non-parametric VaR formula is below:

$$\hat{VaR}(\alpha) = -S*\hat{q}(\alpha)$$

where $S$ is the holdings in the asset or portfolio and $\hat{q}(\alpha)$ is the sample quantile at $\alpha$.

ES is related to VaR. It is the expected return conditional that it is in the worst $\alpha$% of cases.

This non-parametric ES formula is below:

$$\hat{ES}(\alpha) = -S*\frac{\Sigma^n_{i=1}R_i * I(R_i < \hat{q}(\alpha))}{\Sigma^n_{i=1} I(R_i < \hat{q}(\alpha))}$$
where $S$ is the holdings in the asset or portfolio, $R_i$ are returns, $\hat{q}(\alpha)$ is the sample quantile at $\alpha$, and $I$ is an indicator function that is 1 if true and 0 if false.

With that background out of the way, let's apply bootstapping to VaR and ES. I will look at the 10% VaR and ES on a \$1,000,000 portfolio indexed to the S&P 500 for our period interest. Since I am using daily returns, these risk measures apply to a time period of a day.

```{r}
non_parametric_var <- function(returns, prob=0.1, holdings=1000000){
  return(as.numeric(-holdings*quantile(returns, probs=prob)))
}

non_parametric_es <- function(returns, prob=0.1, holdings=1000000){
  below_quantile <- returns < quantile(returns, probs=prob)
  numer <- sum(returns[below_quantile])
  denom <- sum(below_quantile)
  return(-holdings * (numer / denom))
}

var_est <- bootstrap_parallel(returns, non_parametric_var, 100000)
es_est <- bootstrap_parallel(returns, non_parametric_es, 100000)
```

```{r include=FALSE}
my_var_est <- round(var_est$mean, 2)
my_var_se <- round(var_est$se, 2)
my_var_lower <- round(as.numeric(quantile(var_est$estimates, 0.025), 2))
my_var_upper <- round(as.numeric(quantile(var_est$estimates, 0.975), 2))
```

The distribution of bootstrapped estimates of VaR(0.1) on our portfolio are below. Our mean estimate of VaR(0.1) is `r my_var_est` with a standard error of `r my_var_se`. The bootstrapped 95% CI is `r my_var_lower` to `r my_var_upper`.

```{r echo=FALSE}
var_est_df <- as.data.frame(list('VaR'=var_est$estimates))

var_hist <- ggplot(var_est_df, aes(x=VaR)) +
  geom_histogram(aes(y = after_stat(density)),
                 color='darkblue',
                 fill='lightblue',
                 position = 'identity',
                 bins=30) +
  labs(
    title='10% VaR for $1,000,000 w/ 95% Bootstrapped CIs',
    x='VaR (USD)',
    y='density'
    ) +
  geom_vline(xintercept=var_est$mean, linetype='dashed', size=1) +
  geom_vline(xintercept=quantile(var_est$estimates, 0.025), linetype='dashed',
             color='red',
             size=1) +
  geom_vline(xintercept=quantile(var_est$estimates, 0.975), linetype='dashed',
             color='red',
             size=1)

var_hist
```

The histogram is a little lumpy, so perhaps there is not enough data to fill out the left-tail sufficiently. Let's look at ES.

```{r include=FALSE}
my_es_est <- round(es_est$mean, 2)
my_es_se <- round(es_est$se, 2)
my_es_lower <- round(as.numeric(quantile(es_est$estimates, 0.025), 2))
my_es_upper <- round(as.numeric(quantile(es_est$estimates, 0.975), 2))
```

The distribution of bootstrapped estimates of ES(0.1) on our portfolio are below. Our mean estimate of ES(0.1) is `r my_es_est` with a standard error of `r my_es_se`. The bootstrapped 95% CI is `r my_es_lower` to `r my_es_upper`.

```{r echo=FALSE}
es_est_df <- as.data.frame(list('ES'=es_est$estimates))

es_hist <- ggplot(es_est_df, aes(x=ES)) +
  geom_histogram(aes(y = after_stat(density)),
                 color='darkblue',
                 fill='lightblue',
                 position = 'identity',
                 bins=30) +
  labs(
    title='10% Expected Shortfall for $1,000,000 w/ 95% Bootstrapped CIs',
    x='ES (USD)',
    y='density'
    ) +
  geom_vline(xintercept=es_est$mean, linetype='dashed', size=1) +
  geom_vline(xintercept=quantile(es_est$estimates, 0.025), linetype='dashed',
             color='red',
             size=1) +
  geom_vline(xintercept=quantile(es_est$estimates, 0.975), linetype='dashed',
             color='red',
             size=1)

es_hist
```

Our simulated sampling distribution of ES has a nice approximate bell-curve with a bit of a skew towards the right.

I hope this has clarified what bootstrapping is and how to apply it. Thanks for reading!
